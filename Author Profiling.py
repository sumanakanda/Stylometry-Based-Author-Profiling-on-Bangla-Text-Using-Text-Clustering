# -*- coding: utf-8 -*-
"""CSE400.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Icks4hRrtmU-DQdUL2fiYoc0x6IyrkPe
"""

!python --version

from google.colab import drive
drive.mount("/content/drive")

!pip install scikit-learn

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

import time
from tqdm.notebook import tqdm_notebook

import sklearn
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score, adjusted_mutual_info_score, \
                         fowlkes_mallows_score, homogeneity_completeness_v_measure, davies_bouldin_score, calinski_harabasz_score, \
                         mean_squared_error

import re
from string import punctuation

import warnings
warnings.filterwarnings("ignore")

"""## Dataframe creation"""

color_palette = [
  "#0000FF", "#00C850", "#00FF00", "#00FFFF", "#4169E1", "#87CEFA", "#ADFF2F", "#B600C6",
  "#C62E2E", "#F863FF", "#FD0101", "#FF007D", "#FF4BCD", "#FF7F50", "#FFA500", "#FFFF00"
]
custom_cmap = mcolors.ListedColormap(color_palette)

root_dir = "/content/drive/MyDrive/capstone"
ds_1500 = root_dir + "/baad16-4/BAAD16_1500w.csv"
df = pd.read_csv(ds_1500)
df.info()

def tokenize_bangla(text):
  tokenize_list = []
  r_escape = '!"#$%&\'()*+,’।-./:;<=>?@[\\]^_`{|}~'
  r = re.compile(r'([\s\।{}]+)'.format(re.escape(r_escape)))
  list_ = r.split(text)
  list_ = [item.replace(" ", "").replace("\n", "").replace("\t", "") if not item.isspace() else item for item in list_ if item.strip()]
  return list_

df['tokenized_text'] = df['text'].apply(tokenize_bangla)
df['final_text'] = df['tokenized_text'].apply(lambda x: ' '.join(x))
df = df.drop('tokenized_text', axis=1)

label_encoder = LabelEncoder()
df['author_label'] = label_encoder.fit_transform(df['label'])

df.head(792)

df['text'][790]

df['text'][1130]

df['text'][4515]

df['text'][8417]

"""# RUN ALL

## Vectorize
"""

vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), max_df=0.85, sublinear_tf=True)
X = vectorizer.fit_transform(df['final_text'])

"""## Function definitions"""

def getPCA(X_array, n_components):
  pca = PCA(n_components=n_components)
  X_reduced = pca.fit_transform(X_array)
  return X_reduced

def getLSA(X_array, n_components):
  lsa = TruncatedSVD(n_components=n_components)
  X_reduced = lsa.fit_transform(X_array)
  return X_reduced

def getTSNE(X_array, n_components):
  tsne = TSNE(n_components=n_components, perplexity=25.0)
  X_reduced = tsne.fit_transform(X_array)
  return X_reduced

def drawClusters(X_positions, cluster_type, show3d = True):
  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

  scatter1 = ax1.scatter(X_positions[:, 0], X_positions[:, 1], c=df['author_label'], cmap=custom_cmap, alpha=0.6)
  ax1.set_title(f'{cluster_type} Visualization of Text Data (2D)')
  ax1.set_xlabel(f'{cluster_type} Dimension 1')
  ax1.set_ylabel(f'{cluster_type} Dimension 2')

  if (show3d):
    ax2 = fig.add_subplot(122, projection='3d')
    scatter2 = ax2.scatter(X_positions[:, 0], X_positions[:, 1], X_positions[:, 2], c=df['author_label'], cmap=custom_cmap, alpha=0.6)
    ax2.set_title(f'{cluster_type} Visualization of Text Data (3D)')
    ax2.set_xlabel(f'{cluster_type} Dimension 1')
    ax2.set_ylabel(f'{cluster_type} Dimension 2')
    ax2.set_zlabel(f'{cluster_type} Dimension 3')

  plt.show()

def calcKmeans(X_array, n_clusters):
  kmeans = KMeans(n_clusters=n_clusters)
  kmeans.fit(X_array)

  labels = kmeans.labels_
  return labels

def calcDBSCAN(X_array, eps, min_samples):
  dbscan = DBSCAN(eps=eps, min_samples=min_samples)
  dbscan.fit(X_array)

  labels = dbscan.labels_
  return labels

def getExternalValidations(actual_labels, cluster_labels):
  ari = adjusted_rand_score(actual_labels, cluster_labels)
  nmi = normalized_mutual_info_score(actual_labels, cluster_labels)
  ami = adjusted_mutual_info_score(actual_labels, cluster_labels)
  fmi = fowlkes_mallows_score(actual_labels, cluster_labels)
  return ari, nmi, ami, fmi

def getInternalValidations(actual_labels, cluster_labels):
  homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(actual_labels, cluster_labels)
  return homogeneity, completeness, v_measure

def printValidations(actual_labels, cluster_labels, label = ''):
  print(f"{label + ' '}Validations:")
  ari, nmi, ami, fmi = getExternalValidations(actual_labels, cluster_labels)
  homogeneity, completeness, v_measure = getInternalValidations(actual_labels, cluster_labels)
  print(f'ari:{ari:.5f}, nmi:{nmi:.5f}, ami:{ami:.5f}, fmi:{fmi:.5f}')
  print(f'homogeneity:{homogeneity:.5f}, completeness:{completeness:.5f}, v_measure:{v_measure:.5f}')

import math

def calculate_rms(numbers):
  """Calculates the root mean square of a list of numbers.

  Args:
    numbers: A list of numbers.

  Returns:
    The root mean square of the numbers.
  """

  # sum_of_squares = sum(x**2 for x in numbers)
  # mean_of_squares = sum_of_squares / len(numbers)
  # rms = math.sqrt(mean_of_squares)
  # return rms

  # return sum(numbers) / len(numbers), max(numbers)
  return max(numbers)

"""## Clustering without dimension reduction"""

kmean_labels = calcKmeans(X.toarray(), 16)
printValidations(df['label'], kmean_labels, "K-Means")
print()

dbscan_labels = calcDBSCAN(X.toarray(), 2.5, 4)
printValidations(df['label'], dbscan_labels, "DBSCAN")

"""## Clustering with PCA"""

pca = PCA(n_components=3)
X_pca = pca.fit_transform(X.toarray())
drawClusters(X_pca, "PCA")

kmeansV = {
  'ari': [], 'nmi': [], 'ami': [], 'fmi': [],
  'homogeneity': [], 'completeness': [], 'v_measure': []
}
dbscanV = {
  'ari': [], 'nmi': [], 'ami': [], 'fmi': [],
  'homogeneity': [], 'completeness': [], 'v_measure': []
}

for i in tqdm_notebook(range(1, 100)):
  X_pca = getPCA(X.toarray(), 3)
  kmean_labels = calcKmeans(X_pca, 16)
  ari, nmi, ami, fmi = getExternalValidations(df['label'], kmean_labels)
  homogeneity, completeness, v_measure = getInternalValidations(df['label'], kmean_labels)
  kmeansV['ari'].append(ari)
  kmeansV['nmi'].append(nmi)
  kmeansV['ami'].append(ami)
  kmeansV['fmi'].append(fmi)
  kmeansV['homogeneity'].append(homogeneity)
  kmeansV['completeness'].append(completeness)
  kmeansV['v_measure'].append(v_measure)

  dbscan_labels = calcDBSCAN(X_pca, 0.0225, 20)
  ari, nmi, ami, fmi = getExternalValidations(df['label'], dbscan_labels)
  homogeneity, completeness, v_measure = getInternalValidations(df['label'], dbscan_labels)
  dbscanV['ari'].append(ari)
  dbscanV['nmi'].append(nmi)
  dbscanV['ami'].append(ami)
  dbscanV['fmi'].append(fmi)
  dbscanV['homogeneity'].append(homogeneity)
  dbscanV['completeness'].append(completeness)
  dbscanV['v_measure'].append(v_measure)

kmeansV['ari'] = calculate_rms(kmeansV['ari'])
kmeansV['nmi'] = calculate_rms(kmeansV['nmi'])
kmeansV['ami'] = calculate_rms(kmeansV['ami'])
kmeansV['fmi'] = calculate_rms(kmeansV['fmi'])
kmeansV['homogeneity'] = calculate_rms(kmeansV['homogeneity'])
kmeansV['completeness'] = calculate_rms(kmeansV['completeness'])
kmeansV['v_measure'] = calculate_rms(kmeansV['v_measure'])

print("KMEANS avg:")
print(f"ari:{kmeansV['ari']:.5f}, nmi:{kmeansV['nmi']:.5f}, ami:{kmeansV['ami']:.5f}, fmi:{kmeansV['fmi']:.5f}")
print(f"homogeneity:{kmeansV['homogeneity']:.5f}, completeness:{kmeansV['completeness']:.5f}, v_measure:{kmeansV['v_measure']:.5f}")

dbscanV['ari'] = calculate_rms(dbscanV['ari'])
dbscanV['nmi'] = calculate_rms(dbscanV['nmi'])
dbscanV['ami'] = calculate_rms(dbscanV['ami'])
dbscanV['fmi'] = calculate_rms(dbscanV['fmi'])
dbscanV['homogeneity'] = calculate_rms(dbscanV['homogeneity'])
dbscanV['completeness'] = calculate_rms(dbscanV['completeness'])
dbscanV['v_measure'] = calculate_rms(dbscanV['v_measure'])
print("DBSCAN avg:")
print(f"ari:{dbscanV['ari']:.5f}, nmi:{dbscanV['nmi']:.5f}, ami:{dbscanV['ami']:.5f}, fmi:{dbscanV['fmi']:.5f}")
print(f"homogeneity:{dbscanV['homogeneity']:.5f}, completeness:{dbscanV['completeness']:.5f}, v_measure:{dbscanV['v_measure']:.5f}")

kmean_labels = calcKmeans(X_pca, 16)
printValidations(df['label'], kmean_labels, "K-Means")
print()

dbscan_labels = calcDBSCAN(X_pca, 0.02, 8)
n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
print(f"clusters: {n_clusters}")
printValidations(df['label'], dbscan_labels, "DBSCAN")

"""## Clustering with LSA"""

lsa = TruncatedSVD(n_components=3)
X_lsa = lsa.fit_transform(X)
drawClusters(X_lsa, "LSA")

kmeansV = {
  'ari': [], 'nmi': [], 'ami': [], 'fmi': [],
  'homogeneity': [], 'completeness': [], 'v_measure': []
}
dbscanV = {
  'ari': [], 'nmi': [], 'ami': [], 'fmi': [],
  'homogeneity': [], 'completeness': [], 'v_measure': []
}

for i in tqdm_notebook(range(1, 100)):
  X_lsa = getLSA(X.toarray(), 3)
  kmean_labels = calcKmeans(X_lsa, 16)
  ari, nmi, ami, fmi = getExternalValidations(df['label'], kmean_labels)
  homogeneity, completeness, v_measure = getInternalValidations(df['label'], kmean_labels)
  kmeansV['ari'].append(ari)
  kmeansV['nmi'].append(nmi)
  kmeansV['ami'].append(ami)
  kmeansV['fmi'].append(fmi)
  kmeansV['homogeneity'].append(homogeneity)
  kmeansV['completeness'].append(completeness)
  kmeansV['v_measure'].append(v_measure)

  dbscan_labels = calcDBSCAN(X_lsa, 0.0225, 20)
  ari, nmi, ami, fmi = getExternalValidations(df['label'], dbscan_labels)
  homogeneity, completeness, v_measure = getInternalValidations(df['label'], dbscan_labels)
  dbscanV['ari'].append(ari)
  dbscanV['nmi'].append(nmi)
  dbscanV['ami'].append(ami)
  dbscanV['fmi'].append(fmi)
  dbscanV['homogeneity'].append(homogeneity)
  dbscanV['completeness'].append(completeness)
  dbscanV['v_measure'].append(v_measure)

kmeansV['ari'] = calculate_rms(kmeansV['ari'])
kmeansV['nmi'] = calculate_rms(kmeansV['nmi'])
kmeansV['ami'] = calculate_rms(kmeansV['ami'])
kmeansV['fmi'] = calculate_rms(kmeansV['fmi'])
kmeansV['homogeneity'] = calculate_rms(kmeansV['homogeneity'])
kmeansV['completeness'] = calculate_rms(kmeansV['completeness'])
kmeansV['v_measure'] = calculate_rms(kmeansV['v_measure'])

print("KMEANS avg:")
print(f"ari:{kmeansV['ari']:.5f}, nmi:{kmeansV['nmi']:.5f}, ami:{kmeansV['ami']:.5f}, fmi:{kmeansV['fmi']:.5f}")
print(f"homogeneity:{kmeansV['homogeneity']:.5f}, completeness:{kmeansV['completeness']:.5f}, v_measure:{kmeansV['v_measure']:.5f}")

dbscanV['ari'] = calculate_rms(dbscanV['ari'])
dbscanV['nmi'] = calculate_rms(dbscanV['nmi'])
dbscanV['ami'] = calculate_rms(dbscanV['ami'])
dbscanV['fmi'] = calculate_rms(dbscanV['fmi'])
dbscanV['homogeneity'] = calculate_rms(dbscanV['homogeneity'])
dbscanV['completeness'] = calculate_rms(dbscanV['completeness'])
dbscanV['v_measure'] = calculate_rms(dbscanV['v_measure'])
print("DBSCAN avg:")
print(f"ari:{dbscanV['ari']:.5f}, nmi:{dbscanV['nmi']:.5f}, ami:{dbscanV['ami']:.5f}, fmi:{dbscanV['fmi']:.5f}")
print(f"homogeneity:{dbscanV['homogeneity']:.5f}, completeness:{dbscanV['completeness']:.5f}, v_measure:{dbscanV['v_measure']:.5f}")

kmean_labels = calcKmeans(X_lsa, 16)
printValidations(df['label'], kmean_labels, "K-Means")
print()

dbscan_labels = calcDBSCAN(X_lsa, 0.0225, 20)
printValidations(df['label'], dbscan_labels, "DBSCAN")
n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
print(f"clusters: {n_clusters}")

"""## Clustering with TSNE"""

start_time = time.time()

tsne = TSNE(n_components=3, perplexity=30.0)
X_tsne = tsne.fit_transform(X.toarray())

execution_time = time.time() - start_time
print(f"Execution time: {execution_time} seconds")

drawClusters(X_tsne, "T-SNE")

kmeansV = {
  'ari': [], 'nmi': [], 'ami': [], 'fmi': [],
  'homogeneity': [], 'completeness': [], 'v_measure': []
}
dbscanV = {
  'ari': [], 'nmi': [], 'ami': [], 'fmi': [],
  'homogeneity': [], 'completeness': [], 'v_measure': []
}

for i in tqdm_notebook(range(1, 10)):
  X_tsne = getTSNE(X.toarray(), 3)
  kmean_labels = calcKmeans(X_tsne, 16)
  ari, nmi, ami, fmi = getExternalValidations(df['label'], kmean_labels)
  homogeneity, completeness, v_measure = getInternalValidations(df['label'], kmean_labels)
  kmeansV['ari'].append(ari)
  kmeansV['nmi'].append(nmi)
  kmeansV['ami'].append(ami)
  kmeansV['fmi'].append(fmi)
  kmeansV['homogeneity'].append(homogeneity)
  kmeansV['completeness'].append(completeness)
  kmeansV['v_measure'].append(v_measure)

  dbscan_labels = calcDBSCAN(X_tsne, 0.02, 5)
  ari, nmi, ami, fmi = getExternalValidations(df['label'], dbscan_labels)
  homogeneity, completeness, v_measure = getInternalValidations(df['label'], dbscan_labels)
  dbscanV['ari'].append(ari)
  dbscanV['nmi'].append(nmi)
  dbscanV['ami'].append(ami)
  dbscanV['fmi'].append(fmi)
  dbscanV['homogeneity'].append(homogeneity)
  dbscanV['completeness'].append(completeness)
  dbscanV['v_measure'].append(v_measure)

kmeansV['ari'] = calculate_rms(kmeansV['ari'])
kmeansV['nmi'] = calculate_rms(kmeansV['nmi'])
kmeansV['ami'] = calculate_rms(kmeansV['ami'])
kmeansV['fmi'] = calculate_rms(kmeansV['fmi'])
kmeansV['homogeneity'] = calculate_rms(kmeansV['homogeneity'])
kmeansV['completeness'] = calculate_rms(kmeansV['completeness'])
kmeansV['v_measure'] = calculate_rms(kmeansV['v_measure'])
print("KMEANS avg:")
print(f"ari:{kmeansV['ari']:.5f}, nmi:{kmeansV['nmi']:.5f}, ami:{kmeansV['ami']:.5f}, fmi:{kmeansV['fmi']:.5f}")
print(f"homogeneity:{kmeansV['homogeneity']:.5f}, completeness:{kmeansV['completeness']:.5f}, v_measure:{kmeansV['v_measure']:.5f}")

dbscanV['ari'] = calculate_rms(dbscanV['ari'])
dbscanV['nmi'] = calculate_rms(dbscanV['nmi'])
dbscanV['ami'] = calculate_rms(dbscanV['ami'])
dbscanV['fmi'] = calculate_rms(dbscanV['fmi'])
dbscanV['homogeneity'] = calculate_rms(dbscanV['homogeneity'])
dbscanV['completeness'] = calculate_rms(dbscanV['completeness'])
dbscanV['v_measure'] = calculate_rms(dbscanV['v_measure'])
print("DBSCAN avg:")
print(f"ari:{dbscanV['ari']:.5f}, nmi:{dbscanV['nmi']:.5f}, ami:{dbscanV['ami']:.5f}, fmi:{dbscanV['fmi']:.5f}")
print(f"homogeneity:{dbscanV['homogeneity']:.5f}, completeness:{dbscanV['completeness']:.5f}, v_measure:{dbscanV['v_measure']:.5f}")

kmean_labels = calcKmeans(X_tsne, 16)
printValidations(df['label'], kmean_labels, "K-Means")
print()

dbscan_labels = calcDBSCAN(X_tsne, 2.5, 8)
printValidations(df['label'], dbscan_labels, "DBSCAN")
n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
print(f"clusters: {n_clusters}")

"""##2 Step TSNE"""

start_time = time.time()

pca_100 = PCA(n_components=100)
X_pca_for_tsne = pca_100.fit_transform(X.toarray())

pca_tsne = TSNE(n_components=3, perplexity=30.0)
X_pca_tsne = pca_tsne.fit_transform(X_pca_for_tsne)

execution_time = time.time() - start_time
print(f"Execution time: {execution_time} seconds")

drawClusters(X_pca_tsne, "TSNE")

X_pca_for_tsne

author_label_list = df['author_label'].tolist()
author_label_list

# prompt: download X_pca_for_tsne and reuse it as json

import json

# Convert the NumPy array to a list
X_pca_for_tsne_list = X_pca_for_tsne.tolist()

author_label_list = df['author_label'].tolist()

# Create a dictionary to store the data
data = {
    "X_pca_for_tsne": X_pca_for_tsne_list,
    "author_list": author_label_list
}

# Save the data as a JSON file
with open('X_pca_for_tsne.json', 'w') as f:
  json.dump(data, f)

# Download the file
from google.colab import files
files.download('X_pca_for_tsne.json')

kmeans_final = KMeans(n_clusters=16)
kmeans_final.fit(X_pca_tsne)
kmean_labels = kmeans_final.labels_

printValidations(df['label'], kmean_labels, "K-Means")
print()

dbscan_final = DBSCAN(eps=2.5, min_samples=4)
dbscan_final.fit(X_pca_tsne)
dbscan_labels = dbscan_final.labels_

printValidations(df['label'], dbscan_labels, "DBSCAN")
n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
print(f"clusters: {n_clusters}")

start_time = time.time()

lsa_100 = TruncatedSVD(n_components=100)
X_lsa_for_tsne = lsa_100.fit_transform(X)

lsa_tsne = TSNE(n_components=3, perplexity=30.0)
X_lsa_tsne = lsa_tsne.fit_transform(X_lsa_for_tsne)

execution_time = time.time() - start_time
print(f"Execution time: {execution_time} seconds")

drawClusters(X_lsa_tsne, "TSNE")

kmean_labels = calcKmeans(X_lsa_tsne, 16)
printValidations(df['label'], kmean_labels, "K-Means")
print()

dbscan_labels = calcDBSCAN(X_lsa_tsne, 2.5, 4)
printValidations(df['label'], dbscan_labels, "DBSCAN")
n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
print(f"clusters: {n_clusters}")

author_label_map = dict(zip(df['label'], df['author_label']))
author_label_map

color_palette_author_map = {author_label_map[key]: value for key, value in zip(df['label'].unique(), color_palette)}
color_palette_author_map

import json

data = []
for i in range(len(X_pca_tsne)):
  point = {
      "x": float(X_pca_tsne[i][0]),
      "y": float(X_pca_tsne[i][1]),
      "z": float(X_pca_tsne[i][2]),
      "label": int(df['author_label'][i])
  }
  data.append(point)

print(data)

json_data = json.dumps(data)

with open('X_pca_tsne_coordinates.json', 'w') as f:
  f.write(json_data)

from google.colab import files
files.download('X_pca_tsne_coordinates.json')

import joblib

joblib.dump(vectorizer, 'vectorizer.pkl')
files.download('vectorizer.pkl')

joblib.dump(pca_100, 'pca_100.pkl')
files.download('pca_100.pkl')

joblib.dump(pca_tsne, 'pca_tsne.pkl')
files.download('pca_tsne.pkl')

joblib.dump(kmeans_final, 'kmeans.pkl')
files.download('kmeans.pkl')

joblib.dump(dbscan_final, 'dbscan.pkl')
files.download('dbscan.pkl')